{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjedDhKnQe2PWR1t4zoKNs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Snails-tian/mpcVSrl/blob/main/testPetri.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KcWSksMUF9Bx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# 定义连续 Petri 网模拟器（这里只是一个简化的示例）\n",
        "class PetriNetSimulator:\n",
        "    def simulate(self, rate):\n",
        "        # 模拟 Petri 网演变，返回状态\n",
        "        # 这里只是一个示例，实际应用中需要根据系统特性实现\n",
        "        return np.random.rand()  # 返回一个随机状态，实际应该是 Petri 网的状态\n",
        "\n",
        "# 定义深度确定性策略梯度（DDPG）网络\n",
        "class DDPGActor(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DDPGActor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.output_layer = nn.Linear(32, output_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return torch.tanh(self.output_layer(x))\n",
        "\n",
        "class DDPGCritic(nn.Module):\n",
        "    def __init__(self, input_size, action_size):\n",
        "        super(DDPGCritic, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size + action_size, 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.output_layer = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.relu(self.fc1(torch.cat([state, action], dim=1)))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# 定义 DDPG 算法\n",
        "class DDPGAlgorithm:\n",
        "    def __init__(self, input_size, action_size, alpha_actor, alpha_critic, gamma, tau):\n",
        "        self.actor_network = DDPGActor(input_size, action_size)\n",
        "        self.target_actor_network = DDPGActor(input_size, action_size)\n",
        "        self.target_actor_network.load_state_dict(self.actor_network.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor_network.parameters(), lr=alpha_actor)\n",
        "\n",
        "        self.critic_network = DDPGCritic(input_size, action_size)\n",
        "        self.target_critic_network = DDPGCritic(input_size, action_size)\n",
        "        self.target_critic_network.load_state_dict(self.critic_network.state_dict())\n",
        "        self.critic_optimizer = optim.Adam(self.critic_network.parameters(), lr=alpha_critic)\n",
        "\n",
        "        self.huber_loss = nn.SmoothL1Loss()\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "\n",
        "    def update_actor_network(self, state):\n",
        "        self.actor_optimizer.zero_grad()\n",
        "\n",
        "        action = self.actor_network(state)\n",
        "        q_value = -self.critic_network(state, action)\n",
        "        loss = q_value.mean()\n",
        "\n",
        "        loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "    def update_critic_network(self, state, action, reward, next_state):\n",
        "        self.critic_optimizer.zero_grad()\n",
        "\n",
        "        target_action = self.target_actor_network(next_state)\n",
        "        target_q_value = reward + self.gamma * self.target_critic_network(next_state, target_action).detach()\n",
        "\n",
        "        predicted_q_value = self.critic_network(state, action)\n",
        "        loss = self.huber_loss(predicted_q_value, target_q_value)\n",
        "\n",
        "        loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "    def update_target_networks(self):\n",
        "        for target_param, param in zip(self.target_actor_network.parameters(), self.actor_network.parameters()):\n",
        "            target_param.data.copy_((1 - self.tau) * target_param.data + self.tau * param.data)\n",
        "\n",
        "        for target_param, param in zip(self.target_critic_network.parameters(), self.critic_network.parameters()):\n",
        "            target_param.data.copy_((1 - self.tau) * target_param.data + self.tau * param.data)\n",
        "\n",
        "# 训练 DDPG\n",
        "def train_ddpg_algorithm(ddpg_algorithm, petri_net_simulator, num_episodes):\n",
        "    for episode in range(num_episodes):\n",
        "        current_state = torch.rand(5)  # 生成一个简化的连续状态\n",
        "\n",
        "        # 在实际应用中，你需要定义奖励函数等\n",
        "        # 这里只是一个简单的示例\n",
        "\n",
        "        # 使用 DDPG 算法更新网络参数\n",
        "        action = ddpg_algorithm.actor_network(Variable(current_state).unsqueeze(0))\n",
        "        next_state = torch.rand(5)\n",
        "        reward = petri_net_simulator.simulate(action.item())  # 模拟 Petri 网演变，获取奖励\n",
        "        ddpg_algorithm.update_critic_network(Variable(current_state).unsqueeze(0), action, reward, Variable(next_state).unsqueeze(0))\n",
        "        ddpg_algorithm.update_actor_network(Variable(current_state).unsqueeze(0))\n",
        "        ddpg_algorithm.update_target_networks()\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print(f\"Episode: {episode}, Reward: {reward}\")\n",
        "\n",
        "# 示例使用\n",
        "input_size = 5  # 输入状态的维度\n",
        "action_size = 1  # 输出动作的维度，这里假设为速率\n",
        "alpha_actor = 0.001  # Actor 学习率\n",
        "alpha_critic = 0.001  # Critic 学习率\n",
        "gamma = 0.9  # 折扣因子\n",
        "tau = 0.001  # 软更新参数\n",
        "\n",
        "# 创建 DDPG 算法实例\n",
        "ddpg_algorithm = DDPGAlgorithm(input_size, action_size, alpha_actor, alpha_critic, gamma, tau)\n",
        "\n",
        "# 创建连续 Petri 网模拟器\n",
        "petri_net_simulator = PetriNetSimulator()\n",
        "\n",
        "# 训练 DDPG\n",
        "train_ddpg_algorithm(ddpg_algorithm, petri_net_simulator, num_episodes=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPcgvZBYGcgc",
        "outputId": "aa497cc3-cfc4-4f7c-e607-639558b97232"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Reward: 0.11521904659768711\n",
            "Episode: 100, Reward: 0.5910840113156018\n",
            "Episode: 200, Reward: 0.0009260925356918692\n",
            "Episode: 300, Reward: 0.2432375156080261\n",
            "Episode: 400, Reward: 0.5580831001061329\n",
            "Episode: 500, Reward: 0.6052840288340768\n",
            "Episode: 600, Reward: 0.9669169393118394\n",
            "Episode: 700, Reward: 0.07403718259400172\n",
            "Episode: 800, Reward: 0.8647708687974665\n",
            "Episode: 900, Reward: 0.6611007897190799\n"
          ]
        }
      ]
    }
  ]
}